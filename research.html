<!DOCTYPE html>
<html>
    <head>
        <title>Chaeyong Park</title>

        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <meta name="description" content="Chaeyong Park CV">
        <meta name="author" content="Chaeyong Park">
        
        <!-- Compiled and minified CSS -->
        <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;400;600;700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="./static/css/materialize.min.css">
        <link rel="stylesheet" href="./static/css/style.css">

        <!-- JS -->
        <script src="./static/js/jquery.min.js"></script>
        <script src="./static/js/jquery.matchHeight-min.js"></script>
        <script src="./static/js/materialize.min.js"></script>
        <script src="./static/js/main.js"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106422406-2"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-106422406-2');
        </script>
    </head>
    <body>
        <header>
            <!-- Navbar -->
            <nav class="grey lighten-5">
                <div class="container">
                    <div class="nav-wrapper grey lighten-5">
                        <a href="https://chaeyongpark.github.io" class="brand-logo brand-logo-cy black-text">CHAEYONG PARK</a>
                        <ul class="right hide-on-med-and-down navbar-menu">
                            <li><a href="./publications.html" class="black-text">PUBLICATIONS</a></li>
                            <li><a href="./research.html" class="black-text">RESEARCH</a></li>
                        </ul>
                        <a href="#side-modal" class="modal-trigger black-text hide-on-large-only"><i class="material-icons">menu</i></a>
                    </div>
                </div>
            </nav>

            <!-- Modal Structure -->
            <div id="side-modal" class="modal">
                <div class="modal-content main-color">
                    <h4><a href="./publications.html">PUBLICATIONS</a></h4>
                    <hr class="hr">
                    <h4><a href="./research.html">RESEARCH</a></h4>
                </div>
            </div>
        </header>
        <main>
            <!-- Scroll button -->
            <div class="fixed-action-btn hide-on-small-only scroll-btn" style="bottom: 20px; right: 20px">
                <a href="#" class="waves-effect btn-floating btn-large main-color">
                    <i class="large material-icons">expand_less</i>
                </a>
            </div>

            <section class="container section-research">
                <!-- 

                    HAPTIC RENDERING

                -->
                <div class="section-research-header section-padding" id="haptic-rendering">
                    <div class="title">HAPTIC RENDERING</div>
                    <div class="description">Modeling/Rendering Methods for Realistic Haptic Sensations</div>
                </div>


                <!-- WHC2019 -->
                <div class="section-research-body" id="WHC2019-Multimodal-rendering">
                    <hr class="hr" style="margin-top:1rem !important">
                    <div class="title">Realistic Haptic Rendering of Collision Effects Using Multimodal Vibrotactile and Impact Feedback</div>
                    <div class="conf">IEEE World Haptics Conference 2019 (WHC'19)</div>

                    <!-- Abstract -->
                    <div class="research-abstract">
                        <div class="header">ABSTRACT</div>
                        <div class="abstract">This paper addresses the potential benefits of multimodal haptic feedback combining vibrotactile and impact stimuli for the target domain of virtual collision simulation. In this hybrid approach, we complement the limitation of each modality with the advantage of the other modality. We present the design of a hybrid device including both vibration and impact actuators and a physics-based rendering method for realistic collision simulation. We also report a user study carried out to comparatively assess the subjective quality of haptic collision rendering using vibration only, impact only, and multimodal (vibration + impact) stimuli. Experimental results demonstrate that our multimodal approach can contribute to critically expanding the dynamic range of virtual collision simulation, especially between highly stiff objects.</div>
                    </div>

                    <!-- Video -->
                    <div class="research-video row">
                        <div class="col s12 l10 offset-l1">
                            <div class="video-container">
                                <iframe width="853" height="480" src="//www.youtube.com/embed/wjVvP031OLE" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <!-- Citation -->
                    <div class="research-citation">
                        <div class="header">FULL CITATION</div>
                        <div class="card-panel citation">
                            <span class="description">Chaeyong Park, Jaeyoung Park, Seungjae Oh, and Seungmoon Choi. 2019. Realistic Haptic Rendering of Collision Effects Using Multimodal Vibrotactile and Impact Feedback. In Proceedings of the IEEE World Haptics Conference 2019 (WHC '19). IEEE, 449-454. DOI:https://doi.org/10.1109/WHC.2019.8816116</span>
                        </div>
                        <div class="row">
                            <a class="col s12 m3 l2 waves-effect waves-light btn cy-red" href="https://doi.org/10.1109/WHC.2019.8816116"><i class="tiny material-icons left">description</i>PAPER</a>
                            <a class="col s12 m3 l2 waves-effect waves-light btn main-color" id="citation-bibtex"><i class="tiny material-icons left">autorenew</i>BIBTEX</a>
                            <a class="col s12 m4 l3 waves-effect waves-light btn main-color" id="citation-copy"><i class="tiny material-icons left">content_copy</i>COPY TO CLIPBOARD</a>
                        </div>
                    </div>
                </div>

                <!-- AH2018 -->
                <div class="section-research-body" id="AH2018-Viscoelastic-rendering">
                    <hr class="hr">
                    <div class="title">Random Forest for Modeling and Rendering of Viscoelastic Deformable Objects</div>
                    <div class="conf">Asia Haptics 2018</div>

                    <!-- Abstract -->
                    <div class="research-abstract">
                        <div class="header">ABSTRACT</div>
                        <div class="abstract">In the recent past, data-driven approaches have gained importance for modeling and rendering of haptic properties of deformable objects. In this paper, we propose a new data-driven approach based on a well known machine learning technique: random forest. We train the random forest for regression for estimating the input-output mapping between discrete-time interaction data (position/displacement and force) collected on a homogeneous deformable object. Unlike currently existing data-driven approaches, we use at most 1% of the recorded interaction data for the training of the random forest. Even then, the trained random forest model reproduces all the interactions used for the training with a good accuracy. This also provides promising results on unseen data. When employed for haptic rendering, the model estimates smooth and stable interaction forces at an update rate more than 650 Hz.</div>
                    </div>

                    <!-- Video -->
                    <div class="research-video row">
                        <div class="col s12 l10 offset-l1">
                            <div class="video-container">
                                <iframe width="853" height="480" src="//www.youtube.com/embed/2r9Llfw--UY" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <!-- Citation -->
                    <div class="research-citation">
                        <div class="header">FULL CITATION</div>
                        <div class="card-panel citation">
                            <span class="description">Hojun Cha, Amit Bhardwaj, Chaeyong Park, and Seungmoon Choi. 2018. Random Forest for Modeling and Rendering of Viscoelastic Deformable Objects. In Proceedings of the International AsiaHaptics conference (AsiaHaptics '18). Springer, 48-53. DOI:https://doi.org/10.1007/978-981-13-3194-7_10</span>
                        </div>
                        <div class="row">
                            <a class="col s12 m3 l2 waves-effect waves-light btn cy-red" href="https://doi.org/10.1007/978-981-13-3194-7_10"><i class="tiny material-icons left">description</i>PAPER</a>
                            <a class="col s12 m3 l2 waves-effect waves-light btn main-color" id="citation-bibtex"><i class="tiny material-icons left">autorenew</i>BIBTEX</a>
                            <a class="col s12 m4 l3 waves-effect waves-light btn main-color" id="citation-copy"><i class="tiny material-icons left">content_copy</i>COPY TO CLIPBOARD</a>
                        </div>
                    </div>
                </div>


                <!-- 

                    HAPTIC INTERACTION

                -->
                <div style="padding-top: 3.5rem">
                <div class="section-research-header section-padding" id="haptic-AR">
                    <div class="title">HAPTIC INTERACTION & TECHNOLOGY</div>
                    <div class="description">Haptic Interaction and Technology for Desinging Immersive Virtual Environments</div>
                </div>

                <!-- UIST2020 -->
                <div class="section-research-body" id="UIST2020-Programmable-button">
                    <hr class="hr" style="margin-top:1rem !important">
                    <div class="title">Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels</div>
                    <div class="conf">ACM Symposium on User Interface Software and Technology (UIST'20)</div>

                    <!-- Abstract -->
                    <div class="research-abstract">
                        <div class="header">ABSTRACT</div>
                        <div class="abstract">Physical buttons provide clear haptic feedback when pressed and released, but their responses are unvarying. Physical buttons can be powered by force actuators to produce unlimited click sensations, but the cost is substantial. An alternative can be augmenting physical buttons with simple and inexpensive vibration actuators. When pushed, an augmented button generates a vibration overlayed on the button’s original kinesthetic response, under the general framework of haptic augmented reality. We explore the design space of augmented buttons while changing vibration frequency, amplitude, duration, and envelope. We then visualize the perceptual structure of augmented buttons by estimating a perceptual space for 7 physical buttons and 40 augmented buttons. Their sensations are also assessed against adjectives, and results are mapped into the perceptual space to identify meaningful perceptual dimensions. Our results contribute to understanding the benefits and limitations of programmable vibration-augmented physical buttons with emphasis on their feels.</div>
                    </div>

                    <!-- Video -->
                    <div class="research-video row">
                        <div class="col s12 l10 offset-l1">
                            <div class="video-container">
                                <iframe width="853" height="480" src="//www.youtube.com/embed/GlMEwPrSew0" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <!-- Citation -->
                    <div class="research-citation">
                        <div class="header">FULL CITATION</div>
                        <div class="card-panel citation">
                            <span class="description">Chaeyong Park, Jinhyuk Yoon, Seungjae Oh, and Seungmoon Choi. 2020. Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (UIST '20). ACM, 1–14. DOI:https://doi.org/10.1145/3379337.3415837</span>
                        </div>
                        <div class="row">
                            <a class="col s12 m3 l2 waves-effect waves-light btn cy-red" href="http://dx.doi.org/10.1145/3379337.3415837"><i class="tiny material-icons left">description</i>PAPER</a>
                            <a class="col s12 m3 l2 waves-effect waves-light btn main-color" id="citation-bibtex"><i class="tiny material-icons left">autorenew</i>BIBTEX</a>
                            <a class="col s12 m4 l3 waves-effect waves-light btn main-color" id="citation-copy"><i class="tiny material-icons left">content_copy</i>COPY TO CLIPBOARD</a>
                        </div>
                    </div>
                </div>

                <!-- CHI2020 -->
                <div class="section-research-body" id="CHI2020-Phantom-sensation">
                    <hr class="hr">
                    <div class="title">Body-Penetrating Tactile Phantom Sensations</div>
                    <div class="conf">ACM conference on Human Factors in Computing Systems (CHI'20)</div>

                    <!-- Abstract -->
                    <div class="research-abstract">
                        <div class="header">ABSTRACT</div>
                        <div class="abstract">In tactile interaction, a phantom sensation refers to an illusion felt on the skin between two distant points at which vibrations are applied. It can improve the perceptual spatial resolution of tactile stimulation with a few tactors. All phantom sensations reported in the literature act on the skin or out of the body, but no such reports exist for those eliciting sensations penetrating the body. This paper addresses tactile phantom sensations in which two vibration actuators on the dorsal and palmar sides of the hand present an illusion of vibration passing through the hand. We also demonstrate similar tactile illusions for the torso. For optimal design, we conducted user studies while varying vibration frequency, envelope function, stimulus duration, and penetrating direction. Based on the results, we present design guidelines on penetrating phantom sensations for its use in immersive virtual reality applications.</div>
                    </div>

                    <!-- Video -->
                    <div class="research-video row">
                        <div class="col s12 l10 offset-l1">
                            <div class="video-container">
                                <iframe width="853" height="480" src="//www.youtube.com/embed/1o5n98fHkUc" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <!-- Citation -->
                    <div class="research-citation">
                        <div class="header">FULL CITATION</div>
                        <div class="card-panel citation">
                            <span class="description">Jinsoo Kim, Seungjae Oh, Chaeyong Park, and Seungmoon Choi. 2020. Body-Penetrating Tactile Phantom Sensations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). ACM, New York, 1–13. DOI:https://doi.org/10.1145/3313831.3376619</span>
                        </div>
                        <div class="row">
                            <a class="col s12 m3 l2 waves-effect waves-light btn cy-red" href="https://doi.org/10.1145/3313831.3376619"><i class="tiny material-icons left">description</i>PAPER</a>
                            <a class="col s12 m3 l2 waves-effect waves-light btn main-color" id="citation-bibtex"><i class="tiny material-icons left">autorenew</i>BIBTEX</a>
                            <a class="col s12 m4 l3 waves-effect waves-light btn main-color" id="citation-copy"><i class="tiny material-icons left">content_copy</i>COPY TO CLIPBOARD</a>
                        </div>
                    </div>
                </div>
                
                <!-- 

                    Sensing

                -->
                <div style="padding-top: 3.5rem">
                <div class="section-research-header section-padding" id="sensing">
                    <div class="title">SENSING TECHNIQUE</div>
                    <div class="description">Sensing Techniques with Haptic Signals</div>
                </div>

                <!-- CHI2019 -->
                <div class="section-research-body" id="CHI2019-Vibeye">
                    <hr class="hr" style="margin-top:1rem !important">
                    <div class="title">VibEye: Vibration-Mediated Object Recognition for Tangible Interactive Applications</div>
                    <div class="conf">ACM conference on Human Factors in Computing Systems (CHI'19)</div>

                    <!-- Abstract -->
                    <div class="research-abstract">
                        <div class="header">ABSTRACT</div>
                        <div class="abstract">We present VibEye: a vibration-mediated recognition system of objects for tangible interaction. A user holds an object between two fingers wearing VibEye. VibEye triggers a vibration from one finger, and the vibration that has propagated through the object is sensed at the other finger. This vibration includes information about the object's identity, and we represent it using a spectrogram. Collecting the spectrograms of many objects, we formulate the object recognition problem to a classical classification problem among the images. This simple method, when tested with 20 users, shows 92.5% accuracy for 16 objects of the same shape with various materials. This material-based classifier is also extended to the recognition of everyday objects. Lastly, we demonstrate several tangible applications where VibEye provides the needed functionality while enhancing user experiences. VibEye is particularly effective for recognizing objects made of different materials, which is difficult to distinguish by other means such as light and sound.</div>
                    </div>

                    <!-- Video -->
                    <div class="research-video row">
                        <div class="col s12 l10 offset-l1">
                            <div class="video-container">
                                <iframe width="853" height="480" src="//www.youtube.com/embed/rH7KywQ0Z-8" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <!-- Citation -->
                    <div class="research-citation">
                        <div class="header">FULL CITATION</div>
                        <div class="card-panel citation">
                            <span class="description">Seungjae Oh, Gyeore Yun, Chaeyong Park, Jinsoo Kim, and Seungmoon Choi. 2019. VibEye: Vibration-Mediated Object Recognition for Tangible Interactive Applications. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19). ACM, 1–12. DOI:https://doi.org/10.1145/3290605.3300906</span>
                        </div>
                        <div class="row">
                            <a class="col s12 m3 l2 waves-effect waves-light btn cy-red" href="https://doi.org/10.1145/3290605.3300906"><i class="tiny material-icons left">description</i>PAPER</a>
                            <a class="col s12 m3 l2 waves-effect waves-light btn main-color" id="citation-bibtex"><i class="tiny material-icons left">autorenew</i>BIBTEX</a>
                            <a class="col s12 m4 l3 waves-effect waves-light btn main-color" id="citation-copy"><i class="tiny material-icons left">content_copy</i>COPY TO CLIPBOARD</a>
                        </div>
                    </div>
                </div>
            </section>
        </main> 
        <footer class="page-footer main-color white-text">
            <div class="container">
            &copy; 2020. Chaeyong Park
            </div>
        </footer>
    </body>
</html>